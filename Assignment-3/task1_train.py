# -*- coding: utf-8 -*-
"""Assignment-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nmA3WumHvvLAvgtMUf161H2R5g_sKOye
"""

import sys
"""## Hyperparameters"""

isEdgeLength=False
early_stopping=False
toDecay=False
"""
  ?? make it configurable
"""
nIter=20
onColab=False
traffic_data_filename=sys.argv[1]
adjacency_matrix_filename=sys.argv[2]
graph_split_filename=sys.argv[3]
model_name="mcs212138_task1.model"
adjacency_filename="./datasets/"+model_name.split(".")[0]+"_adj.csv"

import torch
"""
  Uncomment if run on colab
"""
if onColab:
  from IPython.display import clear_output
  pt_version = torch.__version__
  print(pt_version)

  # Run it for once before
  # !pip install torch-geometric-temporal
  # clear_output()

import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import copy

import torch.nn.functional as F
from torch_geometric.data import Dataset, Data
from torch_geometric.utils import dense_to_sparse


"""## Loading the dataset"""

traffic_data=pd.read_csv(traffic_data_filename,index_col=0)

traffic_data.head()

nodes=[str(i) for i in traffic_data]

print("No. of nodes: ",len(nodes))
print("No. of time steps: ",len(traffic_data[nodes[0]]))

X=[]
Y=[]
for i in range(traffic_data.shape[0]-1):
  X.append(traffic_data.iloc[i,:])
  Y.append(traffic_data.iloc[i+1,:])
X=np.asarray(X)
Y=np.asarray(Y)

print(X.shape)
print(Y.shape)

"""### Loading the train, test and validation nodes."""

def get_train_test_validation_nodes(filename):
  data=np.load(filename)
  train_nodes=data["train_node_ids"].astype("str")
  test_nodes=data["test_node_ids"].astype("str")
  validation_nodes=data["val_node_ids"].astype("str")
  return train_nodes,test_nodes,validation_nodes

train_nodes,test_nodes,validation_nodes=get_train_test_validation_nodes(graph_split_filename)

train_nodes_mask=torch.zeros(len(nodes),dtype=torch.bool)
test_nodes_mask=torch.zeros(len(nodes),dtype=torch.bool)
validation_nodes_mask=torch.zeros(len(nodes),dtype=torch.bool)

node_mapping={}
inverse_node_mapping={}
for t in enumerate(nodes):
  node_mapping[str(t[1])]=t[0]
  inverse_node_mapping[t[0]]=str(t[1])

for node in train_nodes:
  if node in nodes:
    train_nodes_mask[node_mapping[node]]=True
  else:
    print(f"Node {node} not present as the column of nodes")

for node in test_nodes:
  if node in nodes:
    test_nodes_mask[node_mapping[node]]=True
  else:
    print(f"Node {node} not present as the column of nodes")

for node in validation_nodes:
  if node in nodes:
    validation_nodes_mask[node_mapping[node]]=True
  else:
    print(f"Node {node} not present as the column of nodes")

"""### Loading the graph dense adjacency matrix and making it sparse"""

data_adj=pd.read_csv(adjacency_matrix_filename,index_col=0)
data_adj.fillna(0,inplace=True)

print(data_adj.head())

edge_index,edge_weight=dense_to_sparse(torch.from_numpy(data_adj.values))
"""
  Uncomment according to the dataset
"""
if not isEdgeLength:
  edge_weight=1/edge_weight

"""### Model Class"""

from torch_geometric.nn import SAGEConv, GATv2Conv
class Model(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Test:0
    self.layer1=SAGEConv(1,32,'mean')
    self.layer2=SAGEConv(32,16,'mean')

    ## Test:1
    # self.layer1=GATv2Conv(1, 32, edge_dim=1)
    # self.layer2=GATv2Conv(32, 16, edge_dim=1)
    
    ## Test:2
    # self.layer1=GATConv(1,32,edge_dim=1)
     # self.layer2=GATConv(32,16,edge_dim=1)
    self.linear=torch.nn.Linear(16,1)
  
  def forward(self,d):
    X,edge_index,edge_weight=d.x,d.edge_index,d.edge_weight

    output1=F.relu(self.layer1(X,edge_index))
    output2=F.relu(self.layer2(output1,edge_index))

    output=self.linear(output2)

    return output

"""### Creating batches of data"""

final_data=[]
for i in range(X.shape[0]):
  x=torch.tensor(X[i],dtype=torch.double).reshape(-1,1)
  y=torch.tensor(Y[i],dtype=torch.double).reshape(-1,1)
  d=Data(x=x,y=y,edge_index=edge_index)
  d.train_nodes_mask=train_nodes_mask
  d.test_nodes_mask=test_nodes_mask
  d.validation_nodes_mask=validation_nodes_mask
  d.edge_weight=edge_weight
  final_data.append(d)

from torch_geometric.loader import DataLoader
dataloader=DataLoader(final_data,batch_size=32,shuffle=False)

def calculate_loss(Y_pred,Y,node_mask):
  return torch.mean(torch.abs(Y_pred[node_mask]-Y[node_mask])).item()

"""### Model Instance"""

model=Model().double()

"""### Training the model"""

last_error=1000000000000.0

decay=1.0
for iter in tqdm(range(nIter)):
  model.train()
  for i,data in enumerate(dataloader):
    Y_pred=model(data)
    error=F.mse_loss(Y_pred[data.train_nodes_mask],data.y[data.train_nodes_mask])
    if i%100==0:
      print("The current MSE is: ", error.item()/len(data))
    optimizer=torch.optim.Adam(model.parameters(), lr=0.001*decay)
    optimizer.zero_grad()
    error.backward()
    optimizer.step()
    
  model.eval()
  error=0
  n=len(dataloader.dataset)
  for data in dataloader.dataset:
    Y_pred=model(data)
    error+=calculate_loss(Y_pred,data.y,validation_nodes_mask)
  print(f"The error after {iter+1} iterations is: ",error/n)
  """
    Early stopping the training
  """
  if early_stopping:
    if abs(last_error-error/n)>=0.05:
      last_error=error/n
    else:
      break
  if toDecay and iter%5==0:
    decay=decay/100.0

"""### Saving the model"""

torch.save(model,model_name)


"""### Saving the adjacency matrix for future use"""
cmd=f"cp {adjacency_matrix_filename} {adjacency_filename}"
os.system(cmd)



